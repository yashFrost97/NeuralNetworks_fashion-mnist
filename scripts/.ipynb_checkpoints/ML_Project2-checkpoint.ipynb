{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT 2 - NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTING DATASET AND LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "%matplotlib inline\n",
    "import util_mnist_reader\n",
    "X_train, y_train = util_mnist_reader.load_mnist('../data/fashion', kind='train')\n",
    "X_test, y_test = util_mnist_reader.load_mnist('../data/fashion', kind='t10k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FORMATTING DATA AND ONE-HOT ENCODING Y_TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reshape(60000,1)\n",
    "y_test = y_test.reshape(10000,1)\n",
    "y_train_en = np.zeros([60000,10])\n",
    "i=0\n",
    "for x in y_train_en : \n",
    "    a = y_train[i][0]\n",
    "    x[a] = 1\n",
    "    i+=1\n",
    "    \n",
    "y_test_en = np.zeros([10000,10])\n",
    "i=0\n",
    "for x in y_test_en : \n",
    "    a = y_test[i][0]\n",
    "    x[a] = 1\n",
    "    i+=1\n",
    "    \n",
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. NEURAL NETWORK FROM SCRATCH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 DEFINING WEIGHTS, BIAS, HIDDEN NODES, AND FUNCTIONS NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "h_layer = 300\n",
    "weights1 = np.random.randn(784,h_layer)\n",
    "weights2 = np.random.randn(h_layer,10)\n",
    "bias1 = np.zeros([h_layer,1])\n",
    "bias2 = np.zeros([10,1])\n",
    "loss_arr = []\n",
    "l_rate = 0.5\n",
    "\n",
    "def sigmoid(z1):\n",
    "    return 1/(1+np.exp(-(z1)))\n",
    "\n",
    "def softmax(z2):\n",
    "    exps = np.exp(z2-np.max(z2, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 TRAINING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.311384621433277 0.07926666666666667\n",
      "22.92645974452984 0.12438333333333333\n",
      "21.254221291454325 0.19701666666666667\n",
      "20.093966016880394 0.27175\n",
      "19.301819706340705 0.33668333333333333\n",
      "18.80963744615554 0.3945\n",
      "18.623179726335263 0.44233333333333336\n",
      "18.5375886729297 0.47863333333333336\n",
      "18.41496332802873 0.5022166666666666\n",
      "18.328604501655626 0.51725\n",
      "18.320686177432442 0.5303333333333333\n",
      "18.369586549091522 0.5407166666666666\n",
      "18.42726304162348 0.5496666666666666\n",
      "18.461340120980744 0.5563666666666667\n",
      "18.479597947280695 0.563\n",
      "18.485729427699287 0.5695833333333333\n",
      "18.470526103055818 0.57685\n",
      "18.4365756062286 0.58415\n",
      "18.39458297170608 0.5917833333333333\n",
      "18.34924907166049 0.5988333333333333\n",
      "18.315139517990204 0.6059\n",
      "18.310974636936862 0.6115666666666667\n",
      "18.3386750845125 0.6174\n",
      "18.37497723618122 0.62145\n",
      "18.403836903620025 0.6253833333333333\n",
      "18.39759541838497 0.6303666666666666\n",
      "18.35080886250212 0.63445\n",
      "18.319362000770756 0.6381833333333333\n",
      "18.301173327198093 0.6410166666666667\n",
      "18.28390580577499 0.64425\n",
      "18.26599561122917 0.6449833333333334\n",
      "18.242140244108896 0.6438833333333334\n",
      "18.205221285562082 0.6421666666666667\n",
      "18.15985248206519 0.6401\n",
      "18.117157384764432 0.6394333333333333\n",
      "18.08106529295083 0.63705\n",
      "18.054791511380266 0.6359833333333333\n",
      "18.036769445620983 0.6334333333333333\n",
      "18.02220734366788 0.6304\n",
      "18.013525643021627 0.6279833333333333\n",
      "18.01095357194205 0.6262333333333333\n",
      "18.007214495898314 0.6251333333333333\n",
      "17.99723253928872 0.6243666666666666\n",
      "17.97098869077189 0.62275\n",
      "17.925125986450706 0.62305\n",
      "17.863073152377325 0.6236166666666667\n",
      "17.788213536879674 0.6241666666666666\n",
      "17.70283040871072 0.6258166666666667\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    z1 = np.dot(weights1.T,X_train.T)+bias1\n",
    "    a1 = sigmoid(z1)\n",
    "    der_a1 =np.multiply(a1,(1-a1))\n",
    "    z2 = np.dot(weights2.T,a1)+bias2\n",
    "    a2 = softmax(z2)\n",
    "    dw2 = np.dot((a2-y_train_en.T)/60000,a1.T)\n",
    "    dw1 = np.dot(np.multiply(np.dot(((a2-y_train_en.T)/60000).T,weights2.T).T,der_a1),X_train)\n",
    "    weights2 = weights2-(l_rate*dw2.T)\n",
    "    weights1 = weights1-(l_rate*dw1.T)\n",
    "    bias1 -= l_rate*(np.sum((np.multiply(np.dot(((a2-y_train_en.T)/60000).T,weights2.T).T,der_a1)),axis=1)).reshape(h_layer,1)\n",
    "    bias2 -= l_rate*(np.sum((a2-y_train_en.T)/60000,axis=1)).reshape(10,1)\n",
    "    loss = np.sum(np.multiply(-y_train_en.T,np.log(a2)))/60000\n",
    "    loss_arr.append(loss)\n",
    "    a2 = a2.T\n",
    "    pred=[]\n",
    "    for x in a2:\n",
    "        pred.append(np.argmax(x)) \n",
    "    acc = accuracy_score(y_train,pred)\n",
    "    print(loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_arr = list(range(0,epochs))\n",
    "plt.plot(loss_arr,epoch_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 TESTING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "z11 = np.dot(weights1.T,X_test.T)+bias1\n",
    "layer1 = sigmoid(z11)\n",
    "z22 = np.dot(weights2.T,layer1)+bias2\n",
    "output = softmax(z22)\n",
    "output = output.T\n",
    "final_pred = []\n",
    "for x in output:\n",
    "    final_pred.append(np.argmax(x))\n",
    "acc1 = accuracy_score(y_test,final_pred)\n",
    "conf = confusion_matrix(y_test,final_pred)\n",
    "print(acc1)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. IMPLEMENTING NEURAL NETWORK USING KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim = X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_f = []\n",
    "for x in y_pred:\n",
    "    y_pred_f.append(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test, batch_size=32)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf1 = confusion_matrix(y_test,y_pred_f)\n",
    "print(conf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. IMPLEMENTING CONVOLUTIONAL NEURAL NETWORK USING KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X_train.reshape(-1, 28,28, 1)\n",
    "test_X = X_test.reshape(-1, 28,28, 1)\n",
    "train_X = train_X.astype('float32')\n",
    "test_X = test_X.astype('float32')\n",
    "train_X = train_X / 255.\n",
    "test_X = test_X / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = Sequential()\n",
    "fashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "fashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))\n",
    "fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "# fashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "# fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "# fashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "fashion_model.add(Flatten())\n",
    "fashion_model.add(Dense(128, activation='linear'))\n",
    "fashion_model.add(LeakyReLU(alpha=0.1))                  \n",
    "fashion_model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_train = fashion_model.fit(train_X, y_train, batch_size=32 ,epochs=10, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = fashion_model.predict(test_X)\n",
    "y_pred_cnn_f = []\n",
    "for x in y_pred_cnn:\n",
    "    y_pred_cnn_f.append(np.argmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eval = fashion_model.evaluate(test_X, y_test)\n",
    "print(test_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf2 = confusion_matrix(y_test,y_pred_cnn_f)\n",
    "print(conf2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
